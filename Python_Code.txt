#Research Question: : Where are the highest churn risks located?

#All the packages I imported for this project:
import pandas as pd
pd.set_option('display.max_columns', None)
import missingno as msno
import numpy as np
from fancyimpute import IterativeImputer
from fancyimpute import KNN
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import warnings
from itertools import product
from pyod.models.iforest import IForest
from sklearn.decomposition import PCA
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import folium
from folium.plugins import HeatMap
warnings.filterwarnings('ignore')



#Loading file
df = pd.read_csv(r"C:\Users\alana\OneDrive\Desktop\D206 Assessment\churn_raw_data.csv")



#Checking for Duplicates
print(df.duplicated().value_counts())
df.drop_duplicates()



#Checking for invalid data
#Found that there were negative numbers where there shouldn't have been
print(df.unique())
print(df.describe())
counter = 0
for value in df['Outage_sec_perweek']:
    if value < 0:
        counter += 1
print(counter)
df[df.Outage_sec_perweek < 0] = 0
df.info()

#Typically, you wouldn't want to replace invalids with zeroes but they were all very close to zero relative to the rest of the data and there were only seven invalid responses out of 10,000.



#Replacing Categorical Variables with Dummies
dict = {
        "Techie":{'Yes':1,'No':0},
        'Phone':{'Yes':1,'No':0},
        'Port_modem':{'Yes':1,'No':0},
        'Tablet':{'Yes':1,'No':0},
        'Multiple':{'Yes':1,'No':0},
        'OnlineSecurity':{'Yes':1,'No':0},
        'OnlineBackup':{'Yes':1,'No':0},
        'DeviceProtection':{'Yes':1,'No':0},
        'TechSupport':{'Yes':1,'No':0},
        'StreamingTV':{'Yes':1,'No':0},
        'StreamingMovies':{'Yes':1,'No':0},
        'PaperlessBilling':{'Yes':1,'No':0},
        'Gender':{'Male':1,'Female':0,'Prefer not to answer': np.NAN},
        'InternetService':{'Fiber Optic':1,'DSL':0},
        'Churn':{'Yes':1,'No':0}
        }
df.replace(dict,inplace = True)



#One-Hot Encoding Area, Marital, PaymentMethod, Contract, Employment, and Education (Categorical Variables with Multiple Categories)
df = pd.get_dummies(df, columns = ['Area','Marital','PaymentMethod','Contract','Employment','Education'], drop_first = True, dtype = 'int')



#Checking for nulls
print(df.isna().sum())
print(msno.matrix(df, labels = True))



#Checking for patterns between nulls
msno.heatmap(df)
msno.dendrogram(df)
#There are none, so I can continue with imputation


#KNN and MICE changed the distribution of age from uniform to normal, so I used random sampling instead
count = 0
for value in df['Age'].isna():
    if value == True:
        df['Age'][count] = df['Age'].dropna().sample()
    count += 1
df.info()



#Count NaN values across all rows:
df.isnull().sum(axis = 1).sort_values()
#The highest count of nulls in a row is 7, no need for deletion



#Comparing MICE, KNN, and no imputation error (RMSE) at predicting nulls using a copy of the dataset and LinearRegression to compare RMSEs
#(Had to drop the keys and locational variables temporarily)
df_MICE = df.copy(deep = True)
df_MICE = df_MICE.drop(["Unnamed: 0","CaseOrder","Customer_id","Interaction","City","State","County","Timezone","Job","Lat","Lng","Zip"], axis = 'columns')
MICE_imputer = IterativeImputer()
df_MICE.iloc[:,:] = MICE_imputer.fit_transform(df_MICE)

df_KNN = df.copy(deep = True)
df_KNN = df_KNN.drop(["Unnamed: 0","CaseOrder","Customer_id","Interaction","City","State","County","Timezone","Job","Lat","Lng","Zip"], axis = 'columns')
KNN_imputer = KNN(verbose = 0)
df_KNN.iloc[:,:] = KNN_imputer.fit_transform(df_KNN)

lr = LogisticRegression()
df_copy_no_impute = df.copy(deep = True)
x = df_copy_no_impute.drop(["Unnamed: 0","CaseOrder","Customer_id","Interaction","City","State","County","Timezone","Job","Lat","Lng","Zip"], axis = 'columns').dropna()
y = x["Churn"]
x.drop("Churn", axis = 'columns')
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25, random_state = 0)
lr.fit(x_train,y_train)
predictions = lr.predict(x_test)
accuracy1 = accuracy_score(y_test,predictions)
print(f"no_impute accuracy_score is = {accuracy1}")

lr2 = LogisticRegression()
x2 = df_MICE
y2 = x2["Churn"]
x2.drop("Churn", axis = 'columns')
x_train2,x_test2,y_train2,y_test2 = train_test_split(x2,y2,test_size = 0.25, random_state = 0)
lr2.fit(x_train2,y_train2)
predictions2 = lr2.predict(x_test2)
accuracy2 = accuracy_score(y_test2,predictions2)
print(f"MICE accuracy_score is = {accuracy2}")

lr3 = LogisticRegression()
x3 = df_KNN
y3 = x3["Churn"]
x3.drop("Churn", axis = 'columns')
x_train3,x_test3,y_train3,y_test3 = train_test_split(x3,y3,test_size = .25, random_state = 0)
lr3.fit(x_train3,y_train3)
predictions3 = lr3.predict(x_test3)
accuracy3 = accuracy_score(y_test3,predictions3)
print(f"KNN accuracy_score is = {accuracy3}")


#Checking before and after imputation to confirm distributions were not changed
for col in df_KNN:
    plt.hist(df[col])
    plt.xlabel(col)
    plt.show()
    plt.hist(df_MICE[col])
    plt.xlabel(col)
    plt.show()
    plt.hist(df_KNN[col])
    plt.xlabel(col)
    plt.show()



#Creating function to find potential predictors of the important columns with a large number of nulls
#If there is a very high correlation between two variables, with at least one having many nulls, it may then be better to drop the high-null variable for later regressions
def check_highest_correlation(column_1):
    max_corr = -1
    for col in df_KNN:
        df_KNN_check = df_KNN[[f'{column_1}', f'{col}']]
        df_KNN_check_top_corr = df_KNN_check.dropna()
        if column_1 == col:
            continue
        corr = (df_KNN_check_top_corr[f'{column_1}'].corr(df_KNN_check_top_corr[f'{col}']))
        if corr > max_corr: 
            max_corr = corr
            col_name = col
    return(col_name , max_corr)

for col in df_KNN:
    print(col)
    print(check_highest_correlation(f'{col}'))



#Creating function to find potential predictors of the important columns with a large number of nulls
#Same as above but with lowest correlations. If it were a larger dataset, it would be better to combine them.
def check_lowest_correlation(column_1):
    max_corr = 1
    for col in df_KNN:
        df_KNN_check = df_KNN[[f'{column_1}', f'{col}']]
        df_KNN_check_top_corr = df_KNN_check.dropna()
        if column_1 == col:
            continue
        corr = (df_KNN_check_top_corr[f'{column_1}'].corr(df_KNN_check_top_corr[f'{col}']))
        if corr < max_corr: 
            max_corr = corr
            col_name = col
    return(col_name , max_corr)

for col in df_KNN:
    print(col)
    print(check_lowest_correlation(f'{col}'))



#Identifying and removing Outliers using Isolation Forest and testing with Logistic Regression
#Create function to predict inliers using specified model (IForest)
def outlier_classifier(model,data):
    labels = model.fit_predict(data)
    inliers = data[labels == 0]
    return(inliers)

#Measuring error of hyperparameters of said model using regression given inliers
def regressor(inliers):
    x = inliers.drop("Churn",axis = 1)
    y = inliers["Churn"]
    x_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 1)
    lr0 = LinearRegression()
    lr0.fit(x_train,y_train)
    predictions = lr0.predict(x_test)
    rmse = mean_squared_error(y_test,predictions,squared = False)
    return round(rmse,3)
    
#Finding hyperparameters with the lowest error given hyperparameters
def hyper_parameter_tester(c,n,s,f):
    scores = {}
    x = product(c,n,s,f)
    for c,n,s,f in x:
        IFnsf = IForest(contamination = c, n_estimators = n, max_samples = s, max_features = f, n_jobs = -1)
        resultnsf = outlier_classifier(IFnsf,df_KNN)
        scores[c,n,s,f] = regressor(resultnsf)    
    smallest_rmse = 100
    for key,value in scores.items():
        if value < smallest_rmse:
            smallest_rmse = value
            small_key = key
    return(small_key)

#Lists of sample hyperparameters (Could use range but would take forever and lower ability to determine best one with the following count-based method)
contaminations = [0.01,0.03,0.05]
n_estimators = [250,500,750]
max_samples = [0.3,0.5,0.7]
max_features = [0.3,0.5,0.7]

#Running the test 50 times and appending results to a list
results = []
for int in range(50):
    hyper_parameter_key = hyper_parameter_tester(contaminations,n_estimators,max_samples,max_features)
    results.append(hyper_parameter_key)

#Finding the count of the specified hyperparameters that showed up the most
dictionary = {}
for ans in results:
  dictionary[ans] = dictionary.get(ans, 0) + 1
highest_count = 0
for key,value in dictionary.items():
        if value > highest_count:
            high_count = value
            best_hyperparamters = key
print(dictionary)



#Code to see which hyperparameter combinations showed up the most
for key,value in dictionary.items():
        if value >= 7:
            highest_count = value
            best_hyperparameters = key
            print(best_hyperparameters,highest_count)



#Out of the hyperparameters that showed up the most, which one has the lowest RMSE?
x = outlier_classifier(IForest(contamination = 0.05, n_estimators = 250, max_samples = 0.5, max_features = 0.5),df_KNN)
y = regressor(x)
print(y)



#Checking confidence of outliers predicted by IForest using the hyperparameters that were selected in the previous code
IF = IForest(contamination = 0.03, n_estimators = 750, max_samples = 0.5, max_features = 0.5)
labels = IF.fit_predict(df_KNN)
outliers = df_KNN[IF.labels_ == 1]
print(labels)
outlier_probs = IF.predict_proba(outliers)
for prob in outlier_probs:
    print(prob)



#Dropping the rows with the outliers as marked by IForest
outlier_rows = []
counter = 0
while counter != len(labels):
    if labels[counter] == 1:
        outlier_rows.append(counter)
    counter += 1
print(len(outlier_rows))

df_KNN = df_KNN.drop(outlier_rows)
df_KNN.info()



#Using PCA to explore relationships in the data
scaler = StandardScaler()
df_num = df_KNN.copy(deep = True)
df_num = df_num[['Population','Children','Age','Income','Outage_sec_perweek','Tenure','MonthlyCharge','Bandwidth_GB_Year','item1','item2','item3','item4','item5','item6','item7','item8']]
df_std = pd.DataFrame(scaler.fit_transform(df_num), columns = df_num.columns)
df_std.info()
pca = PCA(n_components = 16)
pca.fit(df_std)
df_pca = pd.DataFrame(pca.transform(df_std), columns = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10','PC11','PC12','PC13','PC14','PC15','PC16'])

plt.plot(pca.explained_variance_ratio_)
plt.xlabel('number of comps')
plt.ylabel('explained variance')

loadings = pd.DataFrame(pca.components_.T, columns = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10','PC11','PC12','PC13','PC14','PC15','PC16'], index = df_num.columns)
print(loadings)



#Using EigenValues to determine which relationships are important
matrix = np.dot(df_std.T,df_std)/df.shape[0]
eigenvalues = [np.dot(eigenvector.T,np.dot(matrix,eigenvector)) for eigenvector in pca.components_]
plt.plot(eigenvalues)
plt.xlabel('number of comps')
plt.ylabel('Eigenvalue')



#Adding a column with all the churn probabilities
LR_final = LogisticRegression()
x = df_KNN.loc[:, df_KNN.columns != 'Churn']
y = df_KNN['Churn']
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = .25, random_state = 0)
LR_final.fit(x_train,y_train)
results = LR_final.predict(x_test)
churn_probs = LR_final.predict_proba(x)
df_KNN['churn_probs'] = churn_probs[:,1]
df_KNN.info()



#Readding all dropped locational variables
new_cols = {'Timezone' : df['Timezone'],
            'City' : df['City'], 
            'State': df['State'], 
            'County' : df['County'], 
            'Job' : df['Job'],
            'Lat' : df['Lat'],
            'Lng' : df['Lng'],
            'Zip' : df['Zip']
            }
df_KNN = df_KNN.assign(**new_cols)
df_KNN.info()


#Filtering for only customers who have not churned
not_yet_churned = df_KNN[df_KNN['Churn'] == 0]
plt.hist(not_yet_churned['churn_probs'])



#Filtering for churn_risks above 0.1 (elbow in the data)
high_churn_risks = not_yet_churned.copy(deep = True)
high_churn_risks = high_churn_risks[high_churn_risks['churn_probs'] > 0.1]
high_churn_risks.info()



#Creating histplots with locational variables against average churn per category
plt.ylabel('Average Churn')
plt.title('Average Churn per Timezone')
high_churn_risks.groupby('Timezone')['churn_probs'].median().nlargest(10).plot.bar()
plt.savefig('Average_Churn_per_Timezone.jpg', bbox_inches = 'tight')
plt.show()

high_churn_risks.groupby('State')['churn_probs'].median().nlargest(10).plot.bar()
plt.ylabel('Average Churn')
plt.title('Average Churn per State')
plt.savefig('Average_Churn_per_State.png', bbox_inches = 'tight')
plt.show()
